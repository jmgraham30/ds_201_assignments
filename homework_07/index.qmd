---
title: "DS 201 Homework 7"
format: 
  html:
    theme: pulse 
    toc: true 
    self-contained: true
---

```{r}
#| include: false

library(tidyverse)
library(tidymodels)
library(ISLR2)

theme_set(theme_minimal(base_size = 13))

tidymodels_prefer()


```

This is Homework Assignment 7 for DS 201. You can view the source code for this assignment on GitHub: [view the source code](https://github.com/jmgraham30/ds_201_assignments/blob/main/homework_07/index.qmd).

For your amusement: Where do hamburgers like to dance? At a meatball!  


## Instructions

In this assignment, you will be working with the `tidymodels` package to train some basic supervised learning models[^1]. We will work with two data sets, one is the `Default` data set from the `ISLR2` data package and the other comes from the [TidyTuesday](https://github.com/rfordatascience/tidytuesday) data repository and has been discussed in the following blog post by the data scientist Julia Silge:

- [Modeling NCAA women's basketball tournament seeds](https://juliasilge.com/blog/ncaa-tuning/). For this data set, we will study a regression problem to predict the number of wins by a team. 


For one of these data sets, we will be interested in a numerical response variable and thus a regression problem, and for the other we will be interested in a categorical response variable and thus a classification problem. 

Here is what you will need to do:

1. Download the Quarto notebook corresponding to this assignment from the course learning management system.

2. Make sure that you have all the necessary R packages installed. The code chunk at the top of the notebook loads all the packages you will need for this assignment. If you get an error message when you try to run this code chunk, make sure that you aren't missing any packages.

3. Go through the notebook and follow the provided prompts. In some cases, you will need to run a code chunk to produce output and then use the results to answer a question or respond to a prompt. In other cases, you will need to write code to produce an appropriate output.

[^1]: Refer to [lesson 8](https://intro-ds.netlify.app/lesson08/) from lecture for an overview of different models and model specifications. 



## Classification Problem for Defaults

We start by examining the data:

```{r}
glimpse(Default)
```


**Exercise 1:** Describe what is contained in this data set. If necessary, run the command `?Default` to learn more. 

With this data, our aim will be to predict which customers will default on their credit card debt.

**Exercise 2:** What type of supervised learning problem is this? What is the response variable? What are the predictor variables and how many are there?

Run the following code to see how many customers in the data set defaulted on their credit card debt:

```{r}
Default %>%
  count(default)
```

**Exercise 3:** What is the baseline accuracy for this data set if we were to always predict that an individual will not default? What is the baseline accuracy for this data set if we were to always predict that an individual will default? 

Before we proceed, we should will do some data exploration and look at the relationships between our predictors and the response. 

**Exercise 4:** Create an appropriate plot to examine the relationship between the `balance` variable and the `default` variable. What do you notice about this relationship? Create an appropriate plot to examine the relationship between the `income` variable and the `default` variable. What do you notice about this relationship? Create an appropriate plot to examine the relationship between the `student` variable and the `default` variable. What do you notice about this relationship?

Since our response is a binary categorical variable, we are interested in classification and a simple model for binary classification is [**logistic regression**](https://en.wikipedia.org/wiki/Logistic_regression).  

The first step in the modeling process is to split the data into a training set and a testing set. We will use the training set to train our model and the testing set to evaluate the performance of our model. Run the following code chunk to split the data into a training set and a testing set:

```{r}
set.seed(123)

default_split <-
  initial_split(Default ,prop = 0.8, strata = default)

default_train <- training(default_split)
default_test <- testing(default_split)
```


Now, we will create a recipe to preprocess the data:


```{r}
log_reg_rec <-
  recipe(default ~ ., data = default_train) %>%
  step_normalize(all_numeric()) %>%
  step_dummy(all_nominal(), -all_outcomes())

```

In the previous code chunk, the function `step_dummy` is used to convert the categorical predictor `student` to a numerical variable and the function `step_normalize` is used to normalize the numerical predictors `balance` and `income`.   

**Exercise 5:** Run the previous code chunk. Then, create a new code chunk and run the command `log_reg_rec %>% prep() %>% bake(default_train)` to see what the data looks like after preprocessing. What do you notice about the data after preprocessing? 

Now, we will fit a model to the training data. Since the response variable is a binary categorical variable, we will fit a logistic regression model to the training data. To do so, we need to create an appropriate model specification. 

```{r}
log_reg_spec <- 
  logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

```

Now we create a workflow that will be used to fit the model. The workflow combines our recipe and model specification.

```{r}
log_reg_wf <-
  workflow() %>%
  add_recipe(log_reg_rec) %>%
  add_model(log_reg_spec)


```

**Exercise 6:** Run the previous code chunk. Then, create a new code chunk and run the command `log_reg_fit <- log_reg_wf %>% fit(default_train)` to fit the model to the training data. 

With a fitted model, we can now make predictions on the testing data. 

**Exercise 7:** Create a new code chunk. In that chunk, copy, paste, and run the following code:

```
log_reg_fit %>%
  predict(default_test, type="class") %>%
  bind_cols(default_test) %>%
  conf_mat(truth = default, estimate = .pred_class)
```

This code makes predictions using the test set, combines the predicted response values with the test data, and constructs a confusion matrix.  What is the accuracy of the model on the testing data? Recall that you can compute accuracy from a confusion matrix by dividing the sum of the diagonal entries by the sum of all the entries in the matrix.


**Exercise 8:** We can also make a plot of the confusion matrix computed in the previous exercise. Create a new code chunk and run the following code:

```
log_reg_fit %>%
  predict(default_test, type="class") %>%
  bind_cols(default_test) %>%
  conf_mat(truth = default, estimate = .pred_class) %>%
  autoplot() 
```

What does this plot tell us about the performance of the model?

**Exercise 9:** How does the accuracy of the logistic regression model compare with the baseline accuracy you stated in Exercise 3? Do you think that this model is a good model? Why or why not?

**Exercise 10:** Create a new model specification and workflow to fit a nearest neighbors model to the training data. To do so, you can use the function `nearest_neighbors` in place of `logistic_reg`.  Compare the performance of your nearest neighbors model to the logistic regression model. Which model performs better? Why do you think that is the case?


## Regression Problem for NCAA Women's Basketball Tournaments

For this section, our modeling goal is to estimate the relationship of expected tournament wins by seed using a data set on NCAA womenâ€™s basketball tournaments.

We start by reading in the data:

```{r}
tournament <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-10-06/tournament.csv")

tournament <- tournament %>%
  drop_na()
```

**Exercise 11:** Use the `glimpse` command to look at the columns in the data set, describe what is contained in this data set. 

The response variable in this data set that we are interested in is `tourney_w` and the predictor variable we are interested in is `seed`.

**Exercise 12:** What type of supervised learning problem is this? 

**Exercise 13:** Plot the relationship between `tourney_w` and `seed`. What do you notice about this relationship? Is the relationship linear?

As always, we will begin by splitting the data into a training set and a testing set. 

```{r}
set.seed(123)

tourney_split <-
  initial_split(tournament ,prop = 0.8, strata = seed)

tourney_train <- training(tourney_split)

tourney_test <- testing(tourney_split)

```

An interesting approach to modeling this data is to use a [**generalized additive model**](https://en.wikipedia.org/wiki/Generalized_additive_model) (GAM). A generalized additive model is a generalization of a linear model that allows for non-linear relationships between the response and the predictors. We can create a model specification for a GAM using the `gen_additive_mod` function. 

```{r}
gam_spec <- 
  gen_additive_mod() %>%
  set_mode("regression")

```


Now, we can fit our model to the training data:

```{r}
gam_fit <- gam_spec %>%
  fit(tourney_w ~ s(seed, k=10),data=tourney_train)

```


**Exercise 14:** Create a new code chunk and predict the model on the test data, combine the model predictions with the test data and store the result in a variable named `tourney_preds`. What is the name of the column in `tourney_preds` that contains the model predictions?

**Exercise 15:** One way to evaluate the performance of a regression model is to compute the root mean squared error (RMSE). The RMSE is defined as the square root of the mean squared error (MSE). The MSE is defined as the average of the squared differences between the observed values and the predicted values. The RMSE has the same units as the response variables and is a measure of the average error of the model. The smaller the RMSE, the better the model. To compute the RMSE for our model, create a new code chunk and run the following code:

```
tourney_preds %>% 
  rmse(truth = tourney_w, estimate = .pred)
```

**Exercise 16:** In order to assess the predictive performance of our model, it's helpful to have a plot of the observed values versus the predicted values. Create a new code chunk and run the following code:

```
tourney_preds %>% 
  ggplot(aes(x = tourney_w, y = .pred)) +
  geom_point() +
  geom_abline(color = "blue",linetype="dashed") 

```

**Exercise 17:** Based on the RMSE value you calculated and the previous plot, how well do you think the model is doing? What are some ways you think the model could be improved?

**Exercise 18:** Instead of fitting a GAM, we could have used the much simpler linear regression. Create a new model specification and workflow to fit a linear regression model to the training data. To do so, you can replace `gen_additive_mod` with the `linear_reg` model specification function. Note that you will also have to change the model formula because the `s(seed,k=10)` part of the model specification for the GAM does not apply for linear regression.  Compare the performance of your linear regression model to the GAM. Which model performs better? Why do you think that is the case?


